{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, data: pd.DataFrame, history: pd.DataFrame, sprints: pd.DataFrame, target: pd.DataFrame, \n",
    "                 threshold=0.1, n_clusters=3, random_state=42):\n",
    "        self.data = data\n",
    "        self.history = history\n",
    "        self.sprints = sprints\n",
    "        self.target = target\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        self.kmeans_successful = MiniBatchKMeans(n_clusters=self.n_clusters, random_state=self.random_state)\n",
    "        self.kmeans_unsuccessful = MiniBatchKMeans(n_clusters=self.n_clusters, random_state=self.random_state)\n",
    "\n",
    "    def select_significant_metrics(self, metric_series):\n",
    "        \"\"\"\n",
    "        Выбирает метрики, которые значительно отличаются от остальных.\n",
    "        :param metric_series: Series с диапазонами метрик.\n",
    "        :return: Список метрик.\n",
    "        \"\"\"\n",
    "        if metric_series.empty:\n",
    "            return []\n",
    "\n",
    "        sorted_metrics = metric_series.sort_values(ascending=False)\n",
    "        max_value = sorted_metrics.iloc[0]\n",
    "        significant_metrics = sorted_metrics[sorted_metrics >= max_value * (1 - self.threshold)]\n",
    "        return significant_metrics.index.tolist()\n",
    "\n",
    "    def cluster_data(self, successful_data, unsuccessful_data):\n",
    "        \"\"\"\n",
    "        Проводит кластеризацию успешных и неуспешных данных.\n",
    "        :param successful_data: DataFrame с успешными спринтами.\n",
    "        :param unsuccessful_data: DataFrame с неуспешными спринтами.\n",
    "        :return: Два DataFrame с добавленными кластерами.\n",
    "        \"\"\"\n",
    "        # Масштабирование данных\n",
    "        features_successful = successful_data.drop(columns=[\"sprint_id\", \"success\"])\n",
    "        features_unsuccessful = unsuccessful_data.drop(columns=[\"sprint_id\", \"success\"])\n",
    "        scaled_successful = self.scaler.fit_transform(features_successful)\n",
    "        scaled_unsuccessful = self.scaler.fit_transform(features_unsuccessful)\n",
    "\n",
    "        # Кластеризация\n",
    "        successful_clusters = self.kmeans_successful.fit_predict(scaled_successful)\n",
    "        unsuccessful_clusters = self.kmeans_unsuccessful.fit_predict(scaled_unsuccessful)\n",
    "\n",
    "        # Добавление кластеров в исходные данные\n",
    "        successful_data = successful_data.copy()\n",
    "        unsuccessful_data = unsuccessful_data.copy()\n",
    "        successful_data[\"cluster\"] = successful_clusters\n",
    "        unsuccessful_data[\"cluster\"] = unsuccessful_clusters\n",
    "\n",
    "        return successful_data, unsuccessful_data\n",
    "\n",
    "    def analyze_clusters(self, successful_data, unsuccessful_data):\n",
    "        \"\"\"\n",
    "        Проводит анализ метрик кластеров для успешных и неуспешных данных.\n",
    "        :param successful_data: DataFrame с кластерами успешных спринтов.\n",
    "        :param unsuccessful_data: DataFrame с кластерами неуспешных спринтов.\n",
    "        :return: Анализ кластеров и наиболее значимые метрики.\n",
    "        \"\"\"\n",
    "        # Расчет средних значений и вариативности для каждого кластера\n",
    "        successful_cluster_means = successful_data.groupby(\"cluster\").mean()\n",
    "        unsuccessful_cluster_means = unsuccessful_data.groupby(\"cluster\").mean()\n",
    "        successful_cluster_variability = successful_data.groupby(\"cluster\").std()\n",
    "        unsuccessful_cluster_variability = unsuccessful_data.groupby(\"cluster\").std()\n",
    "\n",
    "        # Различия между кластерами\n",
    "        successful_diff = successful_cluster_means.max() - successful_cluster_means.min()\n",
    "        unsuccessful_diff = unsuccessful_cluster_means.max() - unsuccessful_cluster_means.min()\n",
    "\n",
    "        # Формирование итогового анализа\n",
    "        cluster_analysis = {\n",
    "            \"Successful Cluster Mean Range\": successful_diff,\n",
    "            \"Successful Cluster Variability\": successful_cluster_variability.mean(),\n",
    "            \"Unsuccessful Cluster Mean Range\": unsuccessful_diff,\n",
    "            \"Unsuccessful Cluster Variability\": unsuccessful_cluster_variability.mean()\n",
    "        }\n",
    "\n",
    "        # Выбор наиболее выделяющихся метрик\n",
    "        successful_top_metrics = self.select_significant_metrics(successful_diff)\n",
    "        unsuccessful_top_metrics = self.select_significant_metrics(unsuccessful_diff)\n",
    "\n",
    "        # Результат анализа\n",
    "        highlighted_metrics = {\n",
    "            \"successful_sprints\": successful_top_metrics,\n",
    "            \"unsuccessful_sprints\": unsuccessful_top_metrics\n",
    "        }\n",
    "\n",
    "        return cluster_analysis, highlighted_metrics\n",
    "\n",
    "    def load_data_from_json(self, json_data: dict) -> pd.DataFrame:\n",
    "        self.df = pd.DataFrame(json_data[\"data\"])\n",
    "        if self.df.empty:\n",
    "            raise ValueError(\"Loaded data is empty.\")\n",
    "\n",
    "        print(\"Data loaded successfully from JSON.\")\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def save_to_json(self, data, file_name=\"highlighted_metrics.json\"):\n",
    "        \"\"\"\n",
    "        Сохраняет данные в JSON-файл.\n",
    "        :param data: Словарь с данными для сохранения.\n",
    "        :param file_name: Имя файла для сохранения.\n",
    "        \"\"\"\n",
    "        with open(file_name, \"w\") as file:\n",
    "            json.dump(data, file)\n",
    "\n",
    "    def preprocess(self) -> pd.DataFrame:\n",
    "    \n",
    "        \"\"\"\n",
    "        Preprocessing + aggregating data\n",
    "        \n",
    "        **sprint_name** - Название спринта(по нему происходит агрегация)\n",
    "\n",
    "        **mean_estimation, median_estimation, sum_estimation** - ['mean', 'median', 'sum']  \n",
    "        # Средняя, медианная и суммарная оценка времени выполнения задачи (в часах)\n",
    "\n",
    "        **avg_completion_time** - 'mean',  # Среднее время выполнения задач\n",
    "\n",
    "        **completion_rate** - # Доля завершённых задач\n",
    "\n",
    "        **rejected_rate** - # Доля отклонённых задач\n",
    "\n",
    "        **defects** - # Количество дефектов\n",
    "\n",
    "        **critical_tasks** - # Количество критических задач\n",
    "\n",
    "        **task_count** - # Общее количество задач\n",
    "\n",
    "        return: aggregated data\n",
    "\n",
    "        \"\"\"\n",
    "        self.history = self.history.reset_index()  # Сброс индекса\n",
    "        columns = self.history.columns.tolist()  # Получаем текущий список колонок\n",
    "        columns = columns[1:] + columns[:1]  # Перемещаем первую колонку в конец\n",
    "        self.history.columns = columns  # Переупорядочиваем названия колонок\n",
    "        if 'index' in self.history.columns:\n",
    "            self.history.drop(columns=['index'], inplace=True)\n",
    "\n",
    "        # Разделение entity_ids в таблице sprints\n",
    "        self.sprints['entity_ids'] = self.sprints['entity_ids'].apply(lambda x: x.strip('{}').split(',') if pd.notnull(x) else [])\n",
    "        sprints_expanded = self.sprints.explode('entity_ids')\n",
    "        sprints_expanded['entity_id'] = sprints_expanded['entity_ids'].astype(float)\n",
    "\n",
    "        # Соединение data и sprints\n",
    "        data_sprints = pd.merge(data, sprints_expanded, on='entity_id', how='left')\n",
    "\n",
    "        # Соединение с history\n",
    "        final_table = pd.merge(data_sprints, self.history, on='entity_id', how='left')\n",
    "\n",
    "        data = final_table.copy()\n",
    "\n",
    "        \n",
    "        data['create_date'] = pd.to_datetime(data['create_date'], errors='coerce')\n",
    "        data['update_date'] = pd.to_datetime(data['update_date'], errors='coerce')\n",
    "        data['sprint_start_date'] = pd.to_datetime(data['sprint_start_date'], errors='coerce')\n",
    "        data['sprint_end_date'] = pd.to_datetime(data['sprint_end_date'], errors='coerce')\n",
    "\n",
    "        data['completion_time'] = (data['update_date'] - data['create_date']).dt.total_seconds() / 3600  # В часах\n",
    "\n",
    "        # Метрики для каждого спринта\n",
    "        agg_sprint_metricks = data.groupby('sprint_name').agg({\n",
    "            'estimation': ['mean', 'median', 'sum'],  # Средняя, медианная и суммарная оценка\n",
    "            'status': lambda x: (x.isin(['Закрыто', 'Готово']).sum()) / len(x),  # Доля завершённых задач\n",
    "            'resolution': lambda x: (x == 'Отклонено').sum() / len(x),  # Доля отклонённых задач\n",
    "            'completion_time': 'mean',  # Среднее время выполнения задач\n",
    "            'type': lambda x: (x == 'Дефект').sum(),  # Количество дефектов\n",
    "            'priority': lambda x: (x == 'Критический').sum(),  # Количество критических задач\n",
    "            'entity_id': 'count'  # Общее количество задач\n",
    "        }).reset_index()\n",
    "\n",
    "        agg_sprint_metricks.columns = ['sprint_name', 'mean_estimation', 'median_estimation', 'sum_estimation',\n",
    "                                'completion_rate', 'rejected_rate', 'avg_completion_time',\n",
    "                                'defects', 'critical_tasks', 'task_count']\n",
    "        \n",
    "        agg_sprint_metricks['target'] = self.target\n",
    "        return agg_sprint_metricks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_conda_ml",
   "language": "python",
   "name": "new_conda_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
